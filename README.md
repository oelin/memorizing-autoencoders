# Memorizing Autoencoders

Research on memorizing autoencoders.

## About

Autoencoders are useful models for feature learning and data compression, however they typically produce poor reconstructions on out-of-distribution data. Memorizing autoencoders (MAEs) are a family of autoencoder models which excel in this task despite intentionally overfitting to a small number of training examples. In this project we aim to understand why MAEs exhibit this surprising behaviour.

## Examples

Reconstructions from an MAE trained on one 1024x1024 image. Compression ratio: 0.95.

<img src='https://github.com/oelin/memorizing-autoencoders/blob/main/images/examples.jpeg'>
